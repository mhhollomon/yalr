/* generated by yalr version 0.3.0 at 2020-11-12 11:19:29 */
#include <iostream>
#include <vector>
#include <regex>
#include <algorithm>
#include <variant>
#include <string_view>
#include <tuple>
#include <set>

#define YALR_DEBUG

/***** verbatim file.top ********/
/***** verbatim file.top ********/

#if defined(YALR_DEBUG)
#  if ! defined(YALR_LDEBUG)
#    define YALR_LDEBUG(msg) { if (debug) \
    std::cerr << msg ; }
#  endif
#  if ! defined(YALR_PDEBUG)
#    define YALR_PDEBUG(msg) { if (debug) \
    std::cerr << msg ; }
#  endif
#else
#  define YALR_LDEBUG(msg)
#  define YALR_PDEBUG(msg)
#endif

namespace YalrParser {

/***** verbatim namespace.top ********/
/***** verbatim namespace.top ********/

enum token_type {
    TOK_regex = 0,
    TOK_alternate = 1,
    TOK_item = 2,
    TOK_closure = 3,
    TOK_plus = 4,
    TOK_optional = 5,
    TOK_atom = 6,
    TOK_LITERAL = 7,
    TOK_0TERM1 = 8,
    TOK_0TERM2 = 9,
    TOK_0TERM3 = 10,
    TOK_0TERM4 = 11,
    TOK_0TERM5 = 12,
    TOK_0TERM6 = 13,
    TOK_regex_prime = 14,
    eoi = 15,
    undef = -1,
    skip = -10,
};

#if defined(YALR_DEBUG)
char const * const token_name[] = {

    "regex",
    "alternate",
    "item",
    "closure",
    "plus",
    "optional",
    "atom",
    "LITERAL",
    "'|'",
    "'*'",
    "'+'",
    "'?'",
    "'('",
    "')'",
    "regex_prime",
    "eoi",
};
#endif

enum state_action { undefined, shift, reduce, accept, error };


using semantic_value = std::variant<
    std::monostate
    >;

struct value_printer {
    std::ostream& strm;
    void operator()(const std::monostate& m) {
        strm << "(void)";
    }
    void operator()(const std::string& s) {
        strm << "'" << s << "'";
    }

    template<typename T>
    void operator()(const T & t) {
        strm << t;
    }
};

struct token {
    token_type t;
    semantic_value v;

    token() {}
    token(token_type tt) : t{tt} {};
    token(token_type tt, semantic_value sv) : t{tt}, v{sv} {};
};

std::ostream& operator<<(std::ostream& strm, token t) {
#if defined(YALR_DEBUG) 
    strm << "{t=(" << t.t << "," << (t.t > -1 ? token_name[t.t] : "undef") << ") v=";
#else
    strm << "{t=" << t.t <<" v=";
#endif
    value_printer vp{strm};
    std::visit(vp, t.v);
    strm << "}";

    return strm;
}

} // namespace


namespace YalrParser {  // for the lexer

std::set<token_type> const global_patterns = {
};


struct dfa_transition_info_t {
    int state_id;
    char low;
    char high;
    int next_state;
};

constexpr std::array<dfa_transition_info_t const, 11> dfa_transitions = {{
    { 0, '(', '(', 6 },
    { 0, ')', ')', 7 },
    { 0, '*', '*', 3 },
    { 0, '+', '+', 4 },
    { 0, '0', '9', 1 },
    { 0, '?', '?', 5 },
    { 0, 'A', 'Z', 1 },
    { 0, '_', '_', 1 },
    { 0, 'a', 'z', 1 },
    { 0, '|', '|', 2 },
    { -9999, '\0', '\0', 0 },
}};

struct dfa_token_info_t {
    int state_id;
    token_type accepted;
    int rank;
};

constexpr std::array<dfa_token_info_t const, 8> dfa_token_info = {{
    { 1, TOK_LITERAL, 15 },
    { 2, TOK_0TERM1, 16 },
    { 3, TOK_0TERM2, 17 },
    { 4, TOK_0TERM3, 18 },
    { 5, TOK_0TERM4, 19 },
    { 6, TOK_0TERM5, 20 },
    { 7, TOK_0TERM6, 21 },
    { -9999, token_type::undef, 0 },
}};

struct dfa_state_info_t {
    int state_id;
    int tokens;
    int transitions;
};

constexpr int dfa_start_state = 0;
constexpr std::array<dfa_state_info_t const, 8> dfa_state_info = {{
    { 0, -1, 0},
    { 1, 0, -1},
    { 2, 1, -1},
    { 3, 2, -1},
    { 4, 3, -1},
    { 5, 4, -1},
    { 6, 5, -1},
    { 7, 6, -1},
}};


template<typename IterType>
class Lexer {

public:
    using iter_type = IterType;

private:
    struct matcher {
        virtual std::pair<bool, int>
        try_match(iter_type first, const iter_type last) const = 0;

        virtual ~matcher() {}
    };

    struct regex_matcher : matcher {
        std::regex pattern;
        regex_matcher(std::string p, const std::regex_constants::syntax_option_type& opt) try : pattern{p, opt} {
            {}
        } catch (std::regex_error &e) {
            std::cerr << "Error when compiling pattern '" << p << "'\n";
            throw e;
        }
        regex_matcher(std::string p) try : pattern{p} {
            {}
        } catch (std::regex_error &e) {
            std::cerr << "Error when compiling pattern '" << p << "'\n";
            throw e;
        }
        virtual std::pair<bool, int>
        try_match(iter_type first, const iter_type last) const override {
            std::match_results<iter_type> mr;
            if (std::regex_search(first, last, mr, pattern, 
                    std::regex_constants::match_continuous)) {
                auto len = mr.length(0);
                return std::make_pair(true, len);
            } else {
                return std::make_pair(false, 0);
            }
        }
    };

    using match_ptr = const std::shared_ptr<const matcher>;

    static inline const std::array<std::tuple<match_ptr, token_type, int>, 0> patterns = {{
    }};

    //******************************************************************************************
    // DFA Matching algorithm
    //******************************************************************************************
    //
    struct dfa_match_results {
        token_type tt = token_type::undef;
        int rank = -1;
        int length = 0;
    };

    //**********************
    // Find a transition for the given state and input.
    // Return new state if found. Return -1 if not.
    //**********************
    int find_transition(int state_id, char input) {

        auto offset = dfa_state_info[state_id].transitions;

        // This state has no transitions
        if (offset < 0) { return -1; }

        auto ptr = &dfa_transitions[offset];

        while (ptr->state_id == state_id) {
            if (ptr->low <= input and ptr->high >= input) {
                return ptr->next_state;
            }
            ++ptr;
        }

        return -1;
    }

    // *****************************
    // ret_val.tt == undef if nothing was matched
    // retval.length will be the number of chars we looked at even 
    // for the no match state.
    //******************************
    dfa_match_results dfa_match(iter_type first, const iter_type last, 
            std::optional<std::set<token_type>> allowed_tokens) {
        dfa_match_results last_match;
        int current_state = dfa_start_state;

        while(true) {
            if (first == last) {
                YALR_LDEBUG("dfa : at eoi - bailing out\n");
                YALR_LDEBUG("dfa : returning {" << last_match.tt << ", " << last_match.length << "}\n");
                return last_match;
            }

            char current_input = *first;
            ++first;

            YALR_LDEBUG( "dfa : current_state = " << current_state <<
                " current_input = '" << current_input << "'\n");
            int new_state = find_transition(current_state, current_input);
            if (new_state == -1) {
                YALR_LDEBUG("dfa : couldn't find a transition\n");
                YALR_LDEBUG("dfa : returning {" << last_match.tt << ", " << last_match.length << "}\n");
                return last_match;
            } else {
                last_match.length += 1;
                auto const token_offset = dfa_state_info[new_state].tokens;
                if (token_offset >= 0) {
                    auto const *token_ptr = &dfa_token_info[token_offset];
                    while (token_ptr->state_id == new_state) {
                        auto tt = token_ptr->accepted;
                        YALR_LDEBUG( "dfa: new state " << new_state << " is accepting tt = " << tt << "\n");
                        if (tt == token_type::skip or not allowed_tokens or allowed_tokens->count(tt)) {
                            YALR_LDEBUG( "dfa: which IS allowed\n");
                            last_match.tt = tt;
                            last_match.rank = token_ptr->rank;
                            break;
                        } else {
                            YALR_LDEBUG( "dfa: but that isn't allowed\n");
                            ++token_ptr;
                        }
                    }

                } else {
                    YALR_LDEBUG("dfa: new state " << new_state << " is NOT accepting\n");
                }

            }
            current_state = new_state;
        }

    }

/***** verbatim lexer.top ********/
/***** verbatim lexer.top ********/
public:
#if defined(YALR_DEBUG)
    bool debug = false;
#endif
    Lexer(iter_type first, const iter_type last) :
        current(first), last(last) {
    }

    virtual token next_token( std::optional<std::set<token_type>> allowed_tokens = std::nullopt) {
        if (current == last) {
            YALR_LDEBUG( "Returning token eoi\n");
            return eoi;
        }

        if (allowed_tokens) {
            allowed_tokens->insert(global_patterns.begin(), global_patterns.end());
        }

        token_type ret_type = undef;
        std::size_t max_len = 0;
        int current_match_rank = -1;
#if defined(YALR_DEBUG)
        if (debug) {
            std::cerr << "lexer: Next few characters: " ;
            auto ptr = current;
            for (int i =0 ; i < 10 && ptr != last; ++i) {
                std::cerr << *ptr;
                ++ptr;
            }
            std::cerr << "\n";

            std::cerr << "lexer: Trying the dfa\n";
        }

#endif

        auto dfa_res = dfa_match(current, last, allowed_tokens);

        if (dfa_res.tt != token_type::undef) {
            ret_type = dfa_res.tt;
            max_len = dfa_res.length;
            current_match_rank = dfa_res.rank;
            YALR_LDEBUG( "lexer: dfa matched for tt = " << ret_type << " length = " << max_len << "\n");
        } else {
            YALR_LDEBUG( "lexer: no dfa match\n");
        }

        for (const auto &[m, tt, rank] : patterns) {
            // if there is a token restriction and we're not on the include list,
            // skip.
            if (allowed_tokens and allowed_tokens->count(tt) == 0) {
                continue;
            }
            YALR_LDEBUG("lexer: Matching for token # " << tt << " rank = " << rank << "\n");
            auto [matched, len] = m->try_match(current, last);
            if (matched) {
                YALR_LDEBUG(" length = " << len << "\n");
                // Override for the same length match if the single matchers came earlier than the dfa match
                if ( std::size_t(len) > max_len or (std::size_t(len) == max_len and rank < current_match_rank)) {
                    max_len = len;
                    ret_type = tt;
                    current_match_rank = rank;
                }
            } else {
                YALR_LDEBUG(" - no match\n");
            }
        }
        if (max_len == 0) {
            current = last;
            return token{undef};
        } else if (ret_type == skip) {
            YALR_LDEBUG("lexer: recursing due to skip\n");
            current += max_len;
            return next_token(allowed_tokens);
        }
        std::string lx{current, current+max_len};
        semantic_value ret_sval;
        switch (ret_type) {
            default :
                /* do nothing */
                break;
        }

        if (debug) {
            std::string lx{current, current+max_len};
            YALR_LDEBUG( "lexer: Returning token = " << ret_type);
            if (ret_type >= 0) {
                YALR_LDEBUG(" (" << token_name[ret_type] << ")")
            }
            YALR_LDEBUG (" lex={" << lx << "}\n");
        }
        current += max_len;
        return token{ret_type, ret_sval};
    }

    // Just needed to make it virtual
    virtual ~Lexer() = default;
private:
    iter_type current;
    const iter_type last;

/***** verbatim lexer.bottom ********/
/***** verbatim lexer.bottom ********/
};

} // namespace YalrParser




namespace YalrParser {  // for the parser

template<typename LexerClass>
class Parser {
public:
    using lexer_class = LexerClass;

private:
    lexer_class& lexer;
    token la;
    int current_state;

    struct parse_stack_entry {
        token tv;
        int         state;
    };

    std::deque<parse_stack_entry> tokstack;

    void printstack() {
        std::cerr << "(st=" << current_state << ")[la= " << la << "]" ;
        for (const auto& x : tokstack) {
            std::cerr << " (" << x.tv;
            std::cerr << ",s=" << x.state;
            std::cerr << ")";
        }
        std::cerr << "\n";
    }

/***** verbatim parser.top start ********/
/***** verbatim parser.top end ********/

/************** reduce functions *****************/
    // A reduce function pointer
    // Returns a semantic value
    using reduce_func_ptr = parse_stack_entry (Parser::*)();

    parse_stack_entry reduce_by_prod0() {
        YALR_PDEBUG( "Reducing by : [0] regex(2) => alternate(4) '|'(16) regex(2)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        tokstack.pop_back();

        
        tokstack.pop_back();

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_regex, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod1() {
        YALR_PDEBUG( "Reducing by : [1] regex(2) => alternate(4)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_regex, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod2() {
        YALR_PDEBUG( "Reducing by : [2] alternate(4) => item(6) alternate(4)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        tokstack.pop_back();

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_alternate, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod3() {
        YALR_PDEBUG( "Reducing by : [3] alternate(4) =>\n");
        parse_stack_entry retval;
        semantic_value sv;

        retval.state = current_state; 

        retval.tv = { TOK_alternate, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod4() {
        YALR_PDEBUG( "Reducing by : [4] item(6) => atom(14)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_item, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod5() {
        YALR_PDEBUG( "Reducing by : [5] item(6) => closure(8)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_item, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod6() {
        YALR_PDEBUG( "Reducing by : [6] item(6) => plus(10)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_item, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod7() {
        YALR_PDEBUG( "Reducing by : [7] item(6) => optional(12)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_item, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod8() {
        YALR_PDEBUG( "Reducing by : [8] closure(8) => atom(14) '*'(17)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        tokstack.pop_back();

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_closure, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod9() {
        YALR_PDEBUG( "Reducing by : [9] plus(10) => atom(14) '+'(18)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        tokstack.pop_back();

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_plus, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod10() {
        YALR_PDEBUG( "Reducing by : [10] optional(12) => atom(14) '?'(19)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        tokstack.pop_back();

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_optional, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod11() {
        YALR_PDEBUG( "Reducing by : [11] atom(14) => LITERAL(15)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_atom, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod12() {
        YALR_PDEBUG( "Reducing by : [12] atom(14) => '('(20) regex(2) ')'(21)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        tokstack.pop_back();

        
        tokstack.pop_back();

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_atom, sv};
        return retval;
    }
    parse_stack_entry reduce_by_prod13() {
        YALR_PDEBUG( "Reducing by : [13] regex_prime(22) => regex(2)\n");
        parse_stack_entry retval;
        semantic_value sv;

        
        retval = tokstack.back(); 
        tokstack.pop_back();

        

        retval.tv = { TOK_regex_prime, sv};
        return retval;
    }
/************** end reduce functions *****************/

/************* parser table *************************/

    struct action_entry {
        // Our current state
        int state_id;
        //  What we just saw. This will only be a true terminal.
        token_type toktype;
        // What we should do
        state_action act_type;
        // shift vars
        int new_state_id;
        // reduce vars
        reduce_func_ptr reduce_func;
    };

    static constexpr action_entry const action_error_entry = {0, token_type::undef, state_action::error, 0, nullptr };


    static inline std::vector<action_entry> state_table = {
        { 0 , TOK_LITERAL , state_action::shift , 8, nullptr},
        { 0, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 0 , TOK_0TERM5 , state_action::shift , 9, nullptr},
        { 0, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 0, eoi, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 1 , eoi , state_action::accept, 0, nullptr },
        { 2 , TOK_0TERM1 , state_action::shift , 10, nullptr},
        { 2, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod1},
        { 2, eoi, state_action::reduce, 0, &Parser::reduce_by_prod1},
        { 3 , TOK_LITERAL , state_action::shift , 8, nullptr},
        { 3, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 3 , TOK_0TERM5 , state_action::shift , 9, nullptr},
        { 3, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 3, eoi, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 4, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod5},
        { 4, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod5},
        { 4, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod5},
        { 4, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod5},
        { 4, eoi, state_action::reduce, 0, &Parser::reduce_by_prod5},
        { 5, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod6},
        { 5, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod6},
        { 5, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod6},
        { 5, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod6},
        { 5, eoi, state_action::reduce, 0, &Parser::reduce_by_prod6},
        { 6, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod7},
        { 6, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod7},
        { 6, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod7},
        { 6, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod7},
        { 6, eoi, state_action::reduce, 0, &Parser::reduce_by_prod7},
        { 7, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod4},
        { 7, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod4},
        { 7 , TOK_0TERM2 , state_action::shift , 12, nullptr},
        { 7 , TOK_0TERM3 , state_action::shift , 13, nullptr},
        { 7 , TOK_0TERM4 , state_action::shift , 14, nullptr},
        { 7, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod4},
        { 7, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod4},
        { 7, eoi, state_action::reduce, 0, &Parser::reduce_by_prod4},
        { 8, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, TOK_0TERM2, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, TOK_0TERM3, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, TOK_0TERM4, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 8, eoi, state_action::reduce, 0, &Parser::reduce_by_prod11},
        { 9 , TOK_LITERAL , state_action::shift , 8, nullptr},
        { 9, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 9 , TOK_0TERM5 , state_action::shift , 9, nullptr},
        { 9, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 9, eoi, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 10 , TOK_LITERAL , state_action::shift , 8, nullptr},
        { 10, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 10 , TOK_0TERM5 , state_action::shift , 9, nullptr},
        { 10, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 10, eoi, state_action::reduce, 0, &Parser::reduce_by_prod3},
        { 11, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod2},
        { 11, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod2},
        { 11, eoi, state_action::reduce, 0, &Parser::reduce_by_prod2},
        { 12, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod8},
        { 12, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod8},
        { 12, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod8},
        { 12, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod8},
        { 12, eoi, state_action::reduce, 0, &Parser::reduce_by_prod8},
        { 13, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod9},
        { 13, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod9},
        { 13, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod9},
        { 13, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod9},
        { 13, eoi, state_action::reduce, 0, &Parser::reduce_by_prod9},
        { 14, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod10},
        { 14, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod10},
        { 14, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod10},
        { 14, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod10},
        { 14, eoi, state_action::reduce, 0, &Parser::reduce_by_prod10},
        { 15 , TOK_0TERM6 , state_action::shift , 17, nullptr},
        { 16, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod0},
        { 16, eoi, state_action::reduce, 0, &Parser::reduce_by_prod0},
        { 17, TOK_LITERAL, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, TOK_0TERM1, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, TOK_0TERM2, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, TOK_0TERM3, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, TOK_0TERM4, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, TOK_0TERM5, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, TOK_0TERM6, state_action::reduce, 0, &Parser::reduce_by_prod12},
        { 17, eoi, state_action::reduce, 0, &Parser::reduce_by_prod12},
    };

    const action_entry &find_action(int state, token_type tt) {
        // binary search
        int bottom = 0;
        int top = state_table.size()-1;
            
        while(true) {
            if (top < bottom) {
                return action_error_entry;
            } else if (top == bottom) {
                if (state_table[top].state_id == state and state_table[top].toktype == tt) {
                    return state_table[top];
                } else {
                    return action_error_entry;
                }
            } else { // top > bottom 
                int index = (top+bottom)/2; // integer math
                auto &current = state_table[index];
                if ( state > current.state_id or (state == current.state_id and tt > current.toktype)) {
                    // index will equal bottom if top and bottom are 1 apart.
                    if (index == bottom) {
                        bottom += 1;
                    } else {
                        bottom = index;
                    }
                } else if (state == current.state_id and tt == current.toktype) {
                    // we hit pay dirt
                    return current;
                } else { // key is < suspect. look in lower half.
                    // integer math above ensures that, if we get here,
                    // index will be strictly less than top.
                    top = index;
                }
            }

        } /// while

        // should never get here. but just to make sure.
        // probably should be an assert.
        return action_error_entry;
    }

    struct goto_entry {
        int state_id;
        token_type toktype;
        int new_state_id;
    };

    std::vector<goto_entry> goto_table {
        { 0, TOK_regex, 1 },
        { 0, TOK_alternate, 2 },
        { 0, TOK_item, 3 },
        { 0, TOK_closure, 4 },
        { 0, TOK_plus, 5 },
        { 0, TOK_optional, 6 },
        { 0, TOK_atom, 7 },
        { 3, TOK_alternate, 11 },
        { 3, TOK_item, 3 },
        { 3, TOK_closure, 4 },
        { 3, TOK_plus, 5 },
        { 3, TOK_optional, 6 },
        { 3, TOK_atom, 7 },
        { 9, TOK_regex, 15 },
        { 9, TOK_alternate, 2 },
        { 9, TOK_item, 3 },
        { 9, TOK_closure, 4 },
        { 9, TOK_plus, 5 },
        { 9, TOK_optional, 6 },
        { 9, TOK_atom, 7 },
        { 10, TOK_regex, 16 },
        { 10, TOK_alternate, 2 },
        { 10, TOK_item, 3 },
        { 10, TOK_closure, 4 },
        { 10, TOK_plus, 5 },
        { 10, TOK_optional, 6 },
        { 10, TOK_atom, 7 },
    };

    int find_goto(int state, token_type tt) {
        // binary search
        int bottom = 0;
        int top = goto_table.size()-1;
            
        while(true) {
            if (top < bottom) {
                return -1;
            } else if (top == bottom) {
                if (goto_table[top].state_id == state and goto_table[top].toktype == tt) {
                    return goto_table[top].new_state_id;
                } else {
                    return -1;
                }
            } else { // top > bottom 
                int index = (top+bottom)/2; // integer math
                auto &current = goto_table[index];
                if ( state > current.state_id or (state == current.state_id and tt > current.toktype)) {
                    // index will equal bottom if top and bottom are 1 apart.
                    if (index == bottom) {
                        bottom += 1;
                    } else {
                        bottom = index;
                    }
                } else if (state == current.state_id and tt == current.toktype) {
                    // we hit pay dirt
                    return current.new_state_id;
                } else { // key is < suspect. look in lower half.
                    // integer math above ensures that, if we get here,
                    // index will be strictly less than top.
                    top = index;
                }
            }

        } /// while

        // should never get here. but just to make sure.
        // probably should be an assert.
        return -1;
    }

    std::vector<std::set<token_type>> valid_terms = {
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            eoi, 
        },
        {
            TOK_0TERM1, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM2, TOK_0TERM3, TOK_0TERM4, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM2, TOK_0TERM3, TOK_0TERM4, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_0TERM1, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
        {
            TOK_0TERM6, 
        },
        {
            TOK_0TERM6, eoi, 
        },
        {
            TOK_LITERAL, TOK_0TERM1, TOK_0TERM2, TOK_0TERM3, TOK_0TERM4, TOK_0TERM5, TOK_0TERM6, eoi, 
        },
    };

public:
#if defined(YALR_DEBUG)
    bool debug = false;
#endif
    Parser(lexer_class& l) : lexer(l){};

    bool doparse() {
        current_state = 0;
        la = lexer.next_token(valid_terms[current_state]);
        bool done = false;
        while(not done) {
#if defined(YALR_DEBUG)
            if (debug) printstack();
#endif
            auto action = find_action(current_state, la.t);
            switch(action.act_type) {
                case state_action::accept :
                    YALR_PDEBUG("$$$$$$$ Accepting $$$$\n");
                    return true;
                case state_action::shift :
                    YALR_PDEBUG("shifting and going to state " << action.new_state_id << "\n");
                    tokstack.push_back({la, current_state});
                    current_state = action.new_state_id;
                    la = lexer.next_token(valid_terms[current_state]);
                    break;
                case state_action::reduce :
                    {
                    auto pse = std::invoke(action.reduce_func, *this);
                    auto new_state = find_goto(pse.state, pse.tv.t);
                    YALR_PDEBUG("Goto - old state = " <<pse.state <<" new state = " << new_state << "\n");
                    tokstack.push_back(pse);
                    current_state = new_state;
                    }
                    break;
                default:
                    YALR_PDEBUG("!!!!! ERROR !!!!");
                    return false;
            }
        }

        return false;
    }

/***** verbatim parser.bottom start ********/
/***** verbatim parser.bottom end ********/

}; // class Parser


} // namespace YalrParser



namespace YalrParser {
/***** verbatim namespace.bottom start ********/
/***** verbatim namespace.bottom end ********/

} // namespace YalrParser

/***** verbatim file.bottom start ********/
/***** verbatim file.bottom end ********/

